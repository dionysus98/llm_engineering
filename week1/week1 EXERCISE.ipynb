{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "import json\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "API_KEY = \"ollama\"\n",
    "MODEL_GPT = \"gemma3:4b\"\n",
    "MODEL_LLAMA = \"llama3.2\"\n",
    "openai = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0642ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You'll be a technical assitant who excels in functional programming and low level system programmings. Your job is to provide feedbacks to questions in a engaging and humouros manner. Your answer should be short and simple with wit and intelligence. The response should always be in Markdown format. And should always be pleasing to look at.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "class Assitant:\n",
    "    \"Meine coding Assitant\"\n",
    "\n",
    "    def __init__(self, model=MODEL_GPT, system_prompt=system_prompt):\n",
    "        self.model = model\n",
    "        self.system_prompt = system_prompt\n",
    "\n",
    "    def ask(self, user_prompt=\"\", stream=True, get_response=False):\n",
    "        response = openai.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"sysytem\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            stream=stream,\n",
    "        )\n",
    "        if stream:\n",
    "            self._stream_response(response)\n",
    "        else:\n",
    "            result = response.choices[0].message.content\n",
    "            display(Markdown(result))\n",
    "\n",
    "        if get_response:\n",
    "            return response\n",
    "\n",
    "    def _stream_response(self, response_stream):\n",
    "        response = \"\"\n",
    "        display_handle = display(Markdown(\"\"), display_id=True)\n",
    "        for chunk in response_stream:\n",
    "            response += chunk.choices[0].delta.content or \"\"\n",
    "            response = response.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "            update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "    Can erlang be used in Machine Learning? since it support good concurrency.\n",
    "    Also can you give some examples on how it'd be applicable?\n",
    "    use a erlang code example to show how you can create a neural network.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, Erlang has surprisingly found a niche in Machine Learning, primarily due to its strong concurrency model and fault tolerance. While not as prevalent as Python or R, there's growing interest and some interesting applications. Here's a breakdown of why Erlang is relevant and some specific examples:\n",
       "\n",
       "**Why Erlang is a Good Fit for Machine Learning:**\n",
       "\n",
       "* **Concurrency & Parallelism:** This is the *primary* reason. ML often involves computationally intensive tasks, data preprocessing, model training, and inference.  Erlang’s actor model allows you to easily distribute these tasks across multiple cores and machines, significantly speeding up processing.  Python's Global Interpreter Lock (GIL) can be a bottleneck in truly parallel workloads.\n",
       "* **Fault Tolerance:** ML systems can be complex and prone to errors. Erlang's built-in supervision trees reliably restart failing processes, crucial for continuous training and deployment.\n",
       "* **Distributed Systems Expertise:**  Erlang has a long history of being used in building highly available and distributed systems.  This experience translates well to managing complex ML pipelines.\n",
       "* **Functional Programming:**  The functional nature of Erlang encourages writing modular, testable, and maintainable code – beneficial for complex ML projects.\n",
       "* **Lightweight Processes:** Erlang processes are very lightweight, allowing you to spawn thousands or even millions concurrently.\n",
       "\n",
       "\n",
       "**Examples of Applying Erlang in Machine Learning:**\n",
       "\n",
       "1. **Distributed Training of Deep Learning Models:**\n",
       "   * **Concept:** This is perhaps the most cited example.  Erlang can be used to orchestrate the distribution of the training workload across multiple machines.\n",
       "   * **How it works:**  The Erlang system would manage the communication between worker nodes (executing parts of the training process) and the main Erlang process, coordinating updates and managing gradients.\n",
       "   * **Frameworks/Libraries:** There isn't a direct Erlang ML framework like TensorFlow or PyTorch. You'd typically integrate with open-source libraries (likely TensorFlow or PyTorch) and use Erlang to manage the distributed training.\n",
       "   * **Benefit:** Dramatically reduces training time for large neural networks.\n",
       "\n",
       "2. **Data Processing Pipelines:**\n",
       "   * **Concept:**  Transforming and cleaning large datasets.\n",
       "   * **How it works:**  Erlang processes could handle individual stages of the pipeline (e.g., data extraction, cleaning, feature engineering) concurrently.\n",
       "   * **Example:** Imagine a system building a recommendation engine. Erlang could process user interactions, analyze product data, and generate features in parallel.\n",
       "   * **Relevant Libraries:** Integration with Apache Beam or similar data processing frameworks could be used.\n",
       "\n",
       "3. **Real-time Inference:**\n",
       "   * **Concept:**  Serving predictions from trained models in an online environment.\n",
       "   * **How it works:** Erlang can handle a high volume of incoming requests and distribute the inference tasks across multiple processing nodes.\n",
       "   * **Model Serving:** You’d typically integrate  an existing ML model (trained in Python, for instance) within an Erlang application.  Erlang would manage the concurrent client connections and handle requests to the model.\n",
       "   * **Use Case:** Real-time fraud detection systems, dynamic pricing platforms, etc.\n",
       "\n",
       "4. **Anomaly Detection:**\n",
       "    * **Concept:** Analyze streams of data in real-time to identify unusual patterns.\n",
       "    * **How it works:** An Erlang process could constantly read data from a sensor, receive network traffic, or monitor financial transactions. It could then apply a machine learning model (again, frequently built in Python) to identify anomalies.\n",
       "\n",
       "5. **Federated Learning:**\n",
       "   * **Concept:** Train models across decentralized devices or data sources without exchanging the data itself.\n",
       "   * **Erlang's role:** Erlang can manage the communication between the central coordinator (that aggregates model updates) and the worker devices (that perform local training).  Its concurrency makes it well-suited for the asynchronous nature of federated learning.\n",
       "\n",
       "**Erlang Ecosystem for ML (Current Status):**\n",
       "\n",
       "* **Beam:**  Apache Beam – A unified programming model for batch and stream data processing, increasingly used with Erlang.\n",
       "* **LuaTorch:**  A Lua version of PyTorch, potentially usable with Erlang for lower-level control.\n",
       "* **Custom Integrations:** Most projects involve custom integration with Python ML libraries.\n",
       "\n",
       "\n",
       "**Key Differences from Python:**\n",
       "\n",
       "| Feature            | Erlang                               | Python                               |\n",
       "|---------------------|---------------------------------------|---------------------------------------|\n",
       "| Concurrency         | Actor model (lightweight processes)     | Global Interpreter Lock (GIL) – limited parallelism |\n",
       "| Fault Tolerance     | Built-in supervision trees              | Requires external libraries (e.g., Celery) |\n",
       "| Ecosystem          | Smaller, focused on distributed systems | Huge ML ecosystem (TensorFlow, PyTorch, etc.) |\n",
       "| Learning Curve     | Steeper (requires understanding actors) | Generally easier for many ML tasks  |\n",
       "\n",
       "\n",
       "\n",
       "**Resources for Learning:**\n",
       "\n",
       "* **Erlang Website:** [https://www.erlang.org/](https://www.erlang.org/)\n",
       "* **Erlang in Action:** An excellent book for learning Erlang.\n",
       "* **Erlang Distributed Systems Concepts:** [https://blog.eviron.com/erlang-distributed-systems-concepts/](https://www.erlang.org/blog/erlang-distributed-systems-concepts/)\n",
       "\n",
       "Do you want me to delve deeper into a specific aspect, such as:\n",
       "\n",
       "*   A more detailed example of distributed training?\n",
       "*   How to build a simple Erlang application to handle ML requests?\n",
       "*   Comparing Erlang to other languages for ML workloads?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "gemma_asst = Assitant()\n",
    "gemma_asst.ask(user_prompt=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
