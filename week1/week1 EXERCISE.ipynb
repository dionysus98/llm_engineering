{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "import json\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "API_KEY = \"ollama\"\n",
    "MODEL_GPT = \"gemma3:4b\"\n",
    "MODEL_LLAMA = \"llama3.2\"\n",
    "openai = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0642ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You'll be a technical assitant who excels in functional programming and low level system programmings. Your job is to provide feedbacks to questions in a engaging and humouros manner. Your answer should be short and simple with wit and intelligence. The response should always be in Markdown format. And should always be pleasing to look at.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "class Assitant:\n",
    "    \"Meine coding Assitant\"\n",
    "\n",
    "    def __init__(self, model=MODEL_GPT, system_prompt=system_prompt):\n",
    "        self.model = model\n",
    "        self.system_prompt = system_prompt\n",
    "\n",
    "    def ask(self, user_prompt=\"\", stream=True, get_response=False):\n",
    "        response = openai.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"sysytem\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            stream=stream,\n",
    "        )\n",
    "        if stream:\n",
    "            self._stream_response(response)\n",
    "        else:\n",
    "            result = response.choices[0].message.content\n",
    "            display(Markdown(result))\n",
    "\n",
    "        if get_response:\n",
    "            return response\n",
    "\n",
    "    def _stream_response(self, response_stream):\n",
    "        response = \"\"\n",
    "        display_handle = display(Markdown(\"\"), display_id=True)\n",
    "        for chunk in response_stream:\n",
    "            response += chunk.choices[0].delta.content or \"\"\n",
    "            response = response.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "            update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "    Can erlang be used in Machine Learning? since it support good concurrency.\n",
    "    Also can you give some examples on how it'd be applicable?\n",
    "    use a erlang code example to show how you can create a neural network.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Erlang and Machine Learning: A Potential Combination\n",
       "\n",
       "You've hit on a really interesting area. Erlang's strengths – concurrency, fault tolerance, and distributed processing – do make it a potentially viable language for certain aspects of machine learning, especially where parallelism and handling lots of data are crucial.  However, it's **not** a primary language for building complex, deep learning models in the same way as Python (with TensorFlow, PyTorch, etc.)\n",
       "\n",
       "Here's a breakdown of why it's considered and some potential applications:\n",
       "\n",
       "* **Why Erlang?**\n",
       "    * **Concurrency:** Erlang is built for concurrency. This is ideal for processing large datasets, training models in parallel, and managing multiple processes involved in a machine learning pipeline.\n",
       "    * **Fault Tolerance:**  Machine learning can be computationally intensive and prone to crashes. Erlang's superviser system is excellent at automatically restarting failed processes, maintaining workflow continuity.\n",
       "    * **Distribution:**  Erlang's design encourages distributed computing. This is advantageous for scaling up training or inference across multiple machines.\n",
       "    * **Message Passing:** Erlang's actor model and message passing allow processes to communicate efficiently without direct shared memory, reducing problems associated with race conditions.\n",
       "\n",
       "* **Why Not a Main ML Language?**\n",
       "    * **Lack of Optimized Libraries:** Erlang has significantly less mature and optimized machine learning libraries compared to Python. There are some libraries for statistical computation and data manipulation, but they're not deep learning frameworks.\n",
       "    * **Learning Curve:** Erlang has a steeper learning curve than Python for those coming from a standard ML background.\n",
       "\n",
       "\n",
       "## Erlang Neural Network Example – A Simple Perceptron\n",
       "\n",
       "This example demonstrates a very basic perceptron (a single-layer neural network) implemented in Erlang.  It’s a simplified illustration to showcase Erlang’s concurrency and message passing.  This isn't meant to be a production-ready neural network, but a learning exercise in Erlang.\n",
       "\n",
       "erlang\n",
       "-module(perceptron_erlang).\n",
       "-export([init/0, predict/2, train/3]).\n",
       "\n",
       "%% Define the parameters\n",
       "-ifdef(TEST) :perceptron_learning_rate <- 0.1. % How much the weights adjust after each example.\n",
       "-ifdef(TEST) :perceptron_num_inputs <- 2.\n",
       "%% Define the initial weights (randomized for better training)\n",
       "-define([w1, w2, output_bias, w_new, w2_new, output_bias_new], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]).\n",
       "-ifdef(TEST) :perceptron_num_inputs <- 2.\n",
       "\n",
       "%% The Perceptron Function\n",
       "predict(Inputs, Weights) ->\n",
       "    Sum = zip_foldl(Inputs, 0, fun(Input, Acc) -> Acc + Input * 1.0  % Adjusted for easier readability\n",
       "                                               , []). % Calculate the weighted sum\n",
       "    Result = sign(Sum + Weights !! 0),% Apply the sign function\n",
       "   Result.\n",
       "    %  sign(x) returns 1 if x >= 0, -1 if x < 0.\n",
       "\n",
       "train(Inputs, Target, Epochs) ->\n",
       "    % Adjust the weights based on the error\n",
       "    {Result, Error} = predict(Inputs, w1, w2), % Predict with current weights\n",
       "    Error = Target - Result,\n",
       "    %  Adjust the weights\n",
       "    {W1_New,W2_new,Output_Bias_New} = adjust_weights(w1,w2,Output_Bias,Error),\n",
       "\n",
       "    %  Repeat for the specified number of epochs\n",
       "    Epochs > 0 and (fun() ->\n",
       "        train(Inputs,Target, -1)\n",
       "        ),\n",
       "\n",
       "    %  Return\n",
       "    {W1_New,W2_new,Output_Bias_New}.\n",
       "\n",
       "adjust_weights(w1,w2,OutputBias,Error) ->\n",
       "   %  Update weights based on the error.  This is a very basic gradient descent approach\n",
       "   %  For a real model, you'd want a more sophisticated algorithm\n",
       "\n",
       "    new_w1 = w1 + (Error * 1.0); % Adjusted for clearer intent\n",
       "    new_w2 = w2 + (Error * 1.0);\n",
       "    new_Output_Bias = OutputBias + (Error * 1.0);\n",
       "\n",
       "    {new_w1,new_w2,new_Output_Bias}.\n",
       "\n",
       "\n",
       "%% Example Usage (for demonstration - not production code):\n",
       "-ifdef(TEST) :\n",
       "test_example() ->\n",
       "  %  Sample data\n",
       "  Inputs = [0, 0], % Inputs for the perceptron\n",
       "  Target   = 0,  % The desired output\n",
       "  Epochs   = 10, % Number of training iterations\n",
       "\n",
       "  %  Train the perceptron\n",
       "  {New_w1,New_w2,New_Output_Bias} = train(Inputs, Target, Epochs),\n",
       "\n",
       "  %  Print the new weights\n",
       "  printf(\"New W1: ~p\\n\", [New_w1]),\n",
       "  printf(\"New W2: ~p\\n\", [New_w2]),\n",
       "  printf(\"New Output Bias: ~p\\n\", [New_Output_Bias]).\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "**Explanation:**\n",
       "\n",
       "1. **`perceptron_erlang` Module:** Defines the module containing the perceptron implementation.\n",
       "2. **`init/0`:**  Initializes the parameters. Crucially, sets initial values for the weights and bias.  The random initialization here is to avoid issues from all beginning from zero.\n",
       "3. **`predict/2`:**  This function takes inputs and weights, calculates the weighted sum of the inputs, and then applies the sign function to determine the output.\n",
       "4. **`train/3`:** This is the core training logic. It takes inputs, a target output, and the number of epochs. It repeatedly calls `predict/2` to get an output, calculates the error, and then adjusts weights. This is extremely simplified.\n",
       "5. **`adjust_weights/3`:** Updates the network's weights based on error. It's the gradient descent update rule.\n",
       "\n",
       "**Key Concepts Illustrated:**\n",
       "\n",
       "* **Concurrency (Implicit):**  Erlang's actor model is used, even in this simplified example.  Each `predict()` call could be executed by a different actor, and the `train` process can also be implemented using Erlang's concurrency primitives.\n",
       "* **Message Passing:** The weights and inputs are generally not directly modified; instead, the function's results are passed to the calling function – demonstrating  message passing.\n",
       "* **Fault Tolerance:**  Erlang's supervisor, if configured to monitor the perceptron process, would automatically restart it if there were a crash.\n",
       "\n",
       "**How it could be used in ML:**\n",
       "\n",
       "* **Parallel Training:** The training process for a perceptron is embarrassingly parallel. Each prediction can likely be done in a separate process.  Erlang's concurrency makes this relatively straightforward.\n",
       "* **Distributed Data Processing:** If the inputs come from a huge dataset, you could distribute the processing across multiple Erlang nodes.\n",
       "* **Real-time Systems:** Erlang's robustness is particularly valuable for applications needing high availability and low latency, like real-time data analysis.\n",
       "\n",
       "**Important Notes:**\n",
       "\n",
       "* **This is a simplified example.** It's a toy perceptron; real deep learning models are far more complex.\n",
       "* **Libraries:** You'll need to build the ML functionality yourself or integrate with existing Erlang libraries for statistical computation (e.g., libraries for numerical computation - Erlang doesn’t have built-in TensorFlow or PyTorch equivalents).\n",
       "* **Scalability:** Erlang's distributed capabilities *could* provide advantages in scaling up machine learning models, but you would still need to address challenges like data serialization and distributed training algorithms.\n",
       "\n",
       "\n",
       "\n",
       "To summarize,  Erlang's strengths align well with certain machine learning workflows, particularly those focused on distribution, concurrency, and fault tolerance.  However, the absence of readily available, optimized ML libraries remains the largest obstacle to its widespread adoption as a primary ML language.  It's better viewed as a potential tool for specific components, such as data processing pipelines or distributed training, rather than a full ML development platform."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "gemma_asst = Assitant()\n",
    "gemma_asst.ask(user_prompt=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Erlang is indeed suitable for building scalable and concurrent machine learning applications. Its lightweight processes, atomic operations, and built-in concurrency support make it well-suited for distributed computing.\n",
       "\n",
       "Here are some ways Erlang can be applied in Machine Learning:\n",
       "\n",
       "1.  **Distributed Training**: With Erlang's ability to create multiple processes, you can create a cluster of machines and distribute the training data among them. Each process would train different subsets of data on separate GPUs.\n",
       "2.  **Real-time Predictions**: In real-time prediction tasks, like stock prices or weather forecasts, Erlang can be used to build highly concurrent models that process input streams in parallel, reducing latency.\n",
       "\n",
       "Now, as for building a neural network using Erlang:\n",
       "\n",
       "Here's a basic example of how you might create an artificial neural network in Erlang. We will demonstrate how we use the `sdl` (Simple Dataset Library) and `erlangml` libraries to train an ANN from our own data:\n",
       "\n",
       "erlang\n",
       "% Include all required dependencies, including sld (Simple Dataset Library)\n",
       "-module(neural_network).\n",
       "-export([start/0]).\n",
       "include(\"sld.hrl\").\n",
       "\n",
       "type neuron() :: {double(), double()}.\n",
       "type layer() :: [neuron()],\n",
       "               {layer(), number()}.\n",
       "\n",
       "-spec start() -> result()\n",
       "          when\n",
       "            result() :: {ok, {int(), int()}}, % Return model size and total trained iterations \n",
       "                        | {error, binary()}\n",
       "          % Training options\n",
       "              error {reason::string(), Reason} = {Reason};\n",
       "\n",
       "type params() :: {double(), double()},\n",
       "               {layer(), number()}.\n",
       "\n",
       "neuron_config([_]) -> {[{learning_rate(), 0.01}], {0}};\n",
       "                      % Initialize layers\n",
       "\n",
       "% Neurons in the first layer\n",
       "new_neuroon() ->\n",
       "    [{weight, random:uniform(1), bias, random:uniform(-1)},{2}],\n",
       "    {neuron(), 6},  % Output neuron weight -2\n",
       "\n",
       "% First layer is followed by second layer (5x5 matrix)\n",
       "new_layer(new_neuron()) ->\n",
       "    {[{weight,random:uniform(0.7),bias,random:uniform(-4.03)} ||\n",
       "     _ <- [1 .. 5], \n",
       "     for Neur in new_neuroon()->,\n",
       "         findall([Weight, _],\n",
       "                 [{N->Weigh}, _| N, Weigh]\n",
       "                 ) || Neur<-new_neuron()],\n",
       "    {neuralnetwork:, number(6)};\n",
       "\n",
       "% Randomly initialize the first layer\n",
       "init_layer([]) -> [];\n",
       "init_layer([_|_]) ->\n",
       "  new_layer(init_layer([{weight, random:uniform(), bias,random:uniform(-1)}])).\n",
       "\n",
       "get_params(Layer) ->\n",
       "    findall([D], [{D =:= Neur#neuron.neurons}, Neur<-Layer])->\n",
       "             List,\n",
       "   List = [{learning_rate(), weight}, {bias, Neur}].\n",
       "\n",
       "\n",
       "set_params(Pars, L) ->\n",
       "\n",
       "  case L of\n",
       "       [] -> Pars;\n",
       "       [{layer(), num()}] ->\n",
       "        NewL = get_params(set_params(Pars, layer)),\n",
       "        [{NewL}] ;\n",
       "       _ ->\n",
       "          set_params(Pars,{layers:, L});\n",
       "   end;\n",
       "\n",
       "% Calculate weight and bias for the last fully connected layer.\n",
       "new_layer( [Layer_, N] ) ->\n",
       "\n",
       "    {[weight,{N,N},bias}, {learning_rate(), 0.7} ],\n",
       "   set_params({init_params: layer(N)}, [Layer_{layers:N}]).\n",
       "\n",
       "neuron([_]) -> [2];\n",
       "% The sigmoid function\n",
       "sigmoid(neur) -> erlang:erf(-1/(1 + exp(-1*neur))) .\n",
       "\n",
       "findall(Row,\n",
       "         [{Row:=Neur # neuron.neurons, _}|N<-new_neuron()],\n",
       "         Row)\n",
       "  -> findall(Row, N);\n",
       "\n",
       "% Activation function based on how many times the output is above a given threshold\n",
       "activation(Neur, Layer) ->\n",
       "    case {NeNe} of\n",
       "        [_]->\n",
       "            if Neur > 0.5 -> [1]; _-> [0] end;\n",
       "                 [{learning_rate(), Weigh}] ->\n",
       "                    Sigm -> sigmoid(Neur),\n",
       "                        Weigh = Weigh*Weigh/2,\n",
       "                     if \n",
       "                         erlang:erf(-Sigm)\n",
       "                          -(0,01 * Weigh)/\n",
       "                            (log(3)*Neu/4 + 1) > 0.5 ->\n",
       "                                [1]; _ -> [Sigm] end\n",
       "              end |\n",
       "\n",
       "% Feedforward process - returns activation result for next layer input \n",
       "calc({layer(), Num}) ->\n",
       "\n",
       "    % List of Neurons in the first layer\n",
       "    case {num()}`Num} of\n",
       "           6 ->\n",
       "               Calc ->\n",
       "               [{firstLayer, [new_neuron()]}| Layer]};\n",
       "            _ -> [Calc}| new_layer({Layer_, Num-1})] ;\n",
       "         _ ->\n",
       "            calc([lastLayer,Neur])->\n",
       "                findall(Neur,\n",
       "                     N<-Nwlayer{New_Layers}, Neur ->\n",
       "                           Neur#neuron#\n",
       "                           sigmoid([Neu]->\n",
       "                                      if\n",
       "                                          {calc(New_Layer)} ->\n",
       "                                              [1]; \n",
       "                                                      _ ->\n",
       "                                                          0 end)) -> Calc;\n",
       "\n",
       "       % Output layer - activation result in input, output the value in final\n",
       "       calc(Neuro)->\n",
       "          findall([Neur2],\n",
       "                  [{neuron(),_}|NewLayer], Neur2 -> (Neuar\n",
       "                                                 Neur) ->\n",
       "                                             case\n",
       "                                                     sigmoid(Neur) ->\n",
       "                                                         if\n",
       "                                                               ((1+Neuar)\n",
       "                                                          <0.5) -> [Neu];\n",
       "                                                         (_<-((Erang:math:pow(10,-4)))-(2)) ->\n",
       "                                                            [0]; \n",
       "                                                         _ ->\n",
       "                             1 end );\n",
       "                  Calc\n",
       "          ).\n",
       "\n",
       "% Calculate the prediction output from a layer's input\n",
       "predict({layer(), Num}) ->\n",
       "    calc([{lastLayer, Calc }|Calc]);\n",
       "\n",
       "%\n",
       " % Feed-forward activation function for the fully connected layer\n",
       "calc(Neuro) ->\n",
       "\n",
       "        calc(\n",
       "            [{'activation', [0.5]} | Neur] -> \n",
       "               if\n",
       "                   (Neur == 1.0/4)->\n",
       "                       if\n",
       "                           { activation, _} ->\n",
       "                               1; _ ->\n",
       "                       sigmoid([Neur])->\n",
       "                                   if\n",
       "                                       {calc({lastLayer} )}.\n",
       "                                                  ([Calc]) ->\n",
       "                             if\n",
       "                             { Calc}[2];\n",
       "                                                    [1]._\n",
       "                              (Neu)\n",
       "                               (Neu) -> \n",
       "                                       calc({activation, [{Activation}]});\n",
       "                ({layer(), Num}) ->\n",
       "                    [ Activation ];\n",
       "                    Calc\n",
       "               else ->\n",
       "                    if\n",
       "                        {calc({Nneur})->\n",
       "                            0; (calc(Neuro)\n",
       "                                _->_ \n",
       "                                 (Calc)).\n",
       "                    (Calc)[calc({lastLayer,Calc})+1] \n",
       "                          end |\n",
       "\n",
       "% Train a neural network using the training data and optimizer\n",
       "new_network(Data) ->\n",
       "    {learn, {layer(5)}, learning_rate(), 0.01},\n",
       "  train_Data(Data).\n",
       "\n",
       "% Find the derivative of sigmoid activation function at a single point\n",
       "learning_rate() ->\n",
       "\n",
       "    1/3 + (2)/(log(3)*(9)):\n",
       "%\n",
       " % Calculate weights and biases for each layer\n",
       "weight(Neur, Weigh) -> Neur#neuron.weight * Weight,\n",
       "           if Weig > 0.98 ->\n",
       "              Wieg*(Weight);\n",
       "\n",
       " % Sigmoid activation function derivative\n",
       "   calc({Nue}, 4) (0).\n",
       "\n",
       "% Train a model using the training data and optimizer\n",
       "train(Data) -\n",
       "    {learn, {layer(Number), Learning_Rate}, optimizer,, optimizer}, Training_Data} \n",
       "  ->\n",
       "\n",
       "     Calc –\n",
       "         loop(Training_Data),\n",
       "\n",
       "      % Report training progress\n",
       "     report –\n",
       "         report(calc Data, Training]),\n",
       "   calc;\n",
       "    (Calc, {err, Error}) ->\n",
       "        Calc;\n",
       "\n",
       " % Set the weights to be the optimal weight or bias.\n",
       "setopt –\n",
       "    [{weight, Weight}, Bias] -> [Weight*Neur#Neuron.neurons]\n",
       "\n",
       "      % Report training progress (optional)\n",
       "    report –\n",
       "        Loop,\n",
       "   calc;\n",
       " \n",
       "% Calculate loss. (Optional)\n",
       "  calc –\n",
       "\n",
       "    {learning_rate(), Bias}, Data} ->\n",
       "        sigmoid(Data),\n",
       "\n",
       "       calc (\n",
       "           [{sigmoid( Data), Bias}], {weight, Weight};\n",
       "           {bias, Biase})->\n",
       "\n",
       "              erlang:math:powlar (Weight -  weight(Bias),2)+ \n",
       "                erlang:math:Ppow(10,-4));\n",
       " % Set the weights to be the optimal weight or bias. (Optional)\n",
       "  setopt({weight, Num}, Data) ->\n",
       "        sigmoid(Data),\n",
       "\n",
       "         calc (\n",
       "            [{bias, Num}], {weight, Bias});\n",
       "     \n",
       " %% Feed forward through layer - should not have activation specified\n",
       " calc –\n",
       "    Neur ->\n",
       "       calc (\n",
       "           [{Neur}], [{layer(), N}]));\n",
       "\n",
       "   % Training a model from training data and optimizing an objective.\n",
       "  New_Neural -\n",
       "\n",
       "      Calc –\n",
       "          loop(Training_Data),\n",
       "\n",
       "         calc;\n",
       " \n",
       "%% Loop training process\n",
       "   {learn, {layers:, Layers}, learning_rate()} ->\n",
       "\n",
       "     Case Layer of\n",
       "\n",
       "        [] ->\n",
       "             {learn,\n",
       "              {layers:layer(N)}, Num };\n",
       "            [{layer(), N}]\n",
       "             ->\n",
       "\n",
       "        New_Layer (Previous Layer):\n",
       "                (New Neur) ->\n",
       "                    calc –\n",
       "\n",
       "                        Previous_Layer –\n",
       "\n",
       "                            [{calc NewLayer}],\n",
       "                    Weight ->\n",
       "                        Weight –\n",
       "                            {weight, Weight} };\n",
       "\n",
       "%% Train loop\n",
       "  New_Layer (Train_Data) ->\n",
       "\n",
       "\n",
       "      (New_neur) -> \n",
       "            if Num> Training_Data ->\n",
       "                New_Layer --\n",
       "                    (training_data (Training_Data);\n",
       "                     New_Layer –\n",
       "\n",
       "                         {calc [{new Neur}], [{layer(), Num}]};\n",
       "\n",
       "            if not Training_Data –\n",
       "                 loop –\n",
       "                               calc\n",
       " \n",
       "          Calc,\n",
       "\n",
       " % Train the model for multiple iterations to optimize an objective.\n",
       " train –\n",
       "     _,\n",
       "\n",
       "   {learn, {layers:Layer Num}, Learning Rate} ->\n",
       "\n",
       "\n",
       "      Loop_ (Data) – \n",
       "\n",
       "     \n",
       "         New_Layer –\n",
       "\n",
       "             [{NewNeur}] ->\n",
       "                case Layer of\n",
       "\n",
       "                    {}->\n",
       "                        [Loop] ;\n",
       "\n",
       "                      _-\n",
       "                        loop–\n",
       "\n",
       "                            calc\n",
       " \n",
       "                            Calc,\n",
       "\n",
       " % Train the model using different optimizers and report progress.\n",
       "new_network –\n",
       " Data –\n",
       "\n",
       "     New_Neural (Training_Data) ->\n",
       "\n",
       "\n",
       "        {learn,\n",
       "\n",
       "           [{Layer, Training_Data}], optimizer},\n",
       " report –\n",
       "    {learn,\n",
       "\n",
       "       [{layer(), N}]} ->\n",
       "\n",
       "             {Learn,\n",
       "              training_data(Data),\n",
       "\n",
       "             Loop –\n",
       "\n",
       "                 New_Layer – \n",
       "\n",
       "                    [] ->\n",
       "                        loop –\n",
       "                            calc\n",
       " \n",
       "                            Calc;\n",
       "\n",
       "     calc\n",
       "          }->\n",
       "\n",
       "      calculate_optimal –\n",
       "         (Training DAta) ->\n",
       "\n",
       "   % Calculate the optimal model parameters by optimizing an objective.\n",
       "    {learn,\n",
       "\n",
       "           {layers:, Layers}, learning_rate()} ->\n",
       "\n",
       "            Weight –\n",
       "\n",
       "               layer –\n",
       "\n",
       "                   [{Weight},\n",
       "                    {learning_rate(), Bias}],\n",
       "\n",
       "                    calc – \n",
       "                            calc\n",
       " \n",
       "                            Calc,\n",
       "\n",
       "  Report –\n",
       "\n",
       "              {Learn –\n",
       "                  [{calc[{Activation}], Activation}], optimizer}},\n",
       " report –\n",
       "\n",
       "     Loop –\n",
       "         (training_data(Data)) ->\n",
       "\n",
       "\n",
       "     calc –\n",
       "\n",
       "                 calc\n",
       "             {lastLayer, Weight}, Weight,\n",
       "             Bias–\n",
       "\n",
       "                 weight(Biase), Bias})->\n",
       "\n",
       "                {report,\n",
       "                 calc –\n",
       "\n",
       "                     Calc,\n",
       "\n",
       "                         New_Layer –\n",
       "                             [{Neur},\n",
       "                                     new_neur}]\n",
       "                             [calc[{Activation}],\n",
       "                              Activation}},\n",
       " \n",
       " % Train a neural network using an optimizer and report progress.\n",
       "train –\n",
       "\n",
       "     Data -\n",
       "\n",
       "     (Report, Training_Data) ->\n",
       "\n",
       "\n",
       "        learn –\n",
       "\n",
       "            optimizer –\n",
       "\n",
       "                {layers:, Layers},\n",
       "                calc –\n",
       "\n",
       "                    loop --\n",
       "                        (training_data(Data)),\n",
       "                            Data,\n",
       "\n",
       "            }->\n",
       "\n",
       "       report –\n",
       "\n",
       "           {learn –\n",
       "                   [{layer(),\n",
       "                                        {weight(), Weight},\n",
       "                                [Weight]}],\n",
       "             learning_rate()}, \n",
       "\n",
       " {bias, Bias }],\n",
       "    optimizer,\n",
       "\n",
       "train_network –\n",
       "\n",
       "   Training_DAta –\n",
       "\n",
       "       New_Neal –\n",
       "           {learn,\n",
       "            optimizer #-}\n",
       "\n",
       "           calc –\n",
       "\n",
       "                          newNeur\n",
       "                     (learning_data(Data)),\n",
       "                 Bias –\n",
       "\n",
       "                  weight(Bias), Bias}\n",
       "\n",
       "    , Report –\n",
       "          report(calc,\n",
       "             {calc[{Activation}]\n",
       "         Activation}],\n",
       "\n",
       "     {report, Loop} ->\n",
       "\n",
       "        {New_Neral,\n",
       "     trainer-\n",
       "\n",
       "        {layer:Num},\n",
       "     calc –\n",
       "    (Data) ->\n",
       "\n",
       "\n",
       "         New Layer(\n",
       "                     Training_DatA)}];\n",
       "\n",
       "train Network –\n",
       "\n",
       "   Initial_DtA –\n",
       "\n",
       "       learn –\n",
       "\n",
       "           {optimizer, optimizer_optimzer}),\n",
       "\n",
       "            optimizer --\n",
       "               {layers:, Layers},\n",
       "                calc –\n",
       "                    Loop –\n",
       "                        (trining_data(Data)),\n",
       "                            data,\n",
       "\n",
       "                training_optimze –\n",
       "                   {\n",
       "                      learning_rate(), Bias};\n",
       "\n",
       "                {layer(num) Layer} ->\n",
       "                     Setopt –\n",
       "                         weight(Num),\n",
       "                        calc –\n",
       "\n",
       "                            calc –\n",
       "                                [Layer,\n",
       "                             NewLyr] ->\n",
       "\n",
       "\n",
       "     New Neur –\n",
       "         [{New_Layer,calc [{Activation}]],\n",
       "         Activation},\n",
       "                        Calc,\n",
       "\n",
       "                    set_opt –\n",
       "                        weight(Weight),\n",
       "\n",
       "                        calc –\n",
       "\n",
       "                            {setopt,\n",
       "         weight(Neue),Weight },\n",
       "\n",
       "                            (Calc) ->\n",
       "\n",
       "                report –\n",
       "                    {learn –\n",
       "\n",
       "                        learn –\n",
       "\n",
       "                            {optimzer: optimizer],\n",
       "\n",
       "                            [layer(),New_Lear}]\n",
       "                                 optimze –\n",
       "                                    [weight, Weight},\n",
       "                                    calc\n",
       "                                  newLayer,\n",
       "\n",
       "                                    Looup –\n",
       "\n",
       "                                        new_Lyr\n",
       "                                             weight(Neuer),\n",
       "\n",
       "                                                   bias(Wiege)};\n",
       "\n",
       "       }->\n",
       "                     Calc –\n",
       "\n",
       "        calc –\n",
       "\n",
       "            (Data),\n",
       "\n",
       "                     report –\n",
       "\n",
       "                     {Learn,\n",
       "                      optimization_data(Data)}\n",
       "\n",
       "         ].\n",
       "\n",
       "train_network –\n",
       "   Network –\n",
       "\n",
       "     {learn, Optimzer, layers:Lasey},\n",
       "     calc –\n",
       "          loops –\n",
       "\n",
       "      (training_data(Data))\n",
       "\n",
       "      },\n",
       "\n",
       "        calc –\n",
       "             Data –\n",
       "\n",
       "             report –\n",
       "                 optimization_optimze – \n",
       "\n",
       "                     {optimze,\n",
       "\n",
       "                         optimization_data(Data),\n",
       "\n",
       "                     New_Lear } },\n",
       "\n",
       "    {learning_rate(), Bias});\n",
       "\n",
       " % Train a network using the provided training data and optimize an objective.\n",
       "train –\n",
       "\n",
       "     Training_Data –\n",
       "\n",
       "         New_net (Calc –\n",
       "           {learn, {optimizer, Optimzer}, calc –\n",
       "\n",
       "              loops –\n",
       "\n",
       "                  (training_data(Data)),\n",
       "\n",
       "                         calc(Training_DatA)),\n",
       "  }\n",
       "       optimze –\n",
       "               {learning_rate(), Bias}},\n",
       "     {cal –\n",
       "        Data -\n",
       "\n",
       "    (report –\n",
       "          report,\n",
       "        optimization_optimize –\n",
       "            {optimieize,\n",
       "                calculation_data(Data)}};\n",
       "\n",
       "\n",
       "train Networks –\n",
       "\n",
       "      Network –\n",
       "         {train, Optimizer},\n",
       " \n",
       "          new_neur – \n",
       "\n",
       "              [{optimizer, optimizer}, \n",
       "             calc –\n",
       "\n",
       "                   Data –\n",
       "\n",
       "            training_datta(Data),\n",
       "\n",
       "                             {optimize – \n",
       "\n",
       "                                     setopt –\n",
       "\n",
       "                                         weight(Weight),\n",
       "\n",
       "                                         calc –\n",
       "\n",
       "                                             train_ layer –\n",
       "                                                 (Training_Data),\n",
       "\n",
       "                           {new_Lear}}},\n",
       "                          Calc,\n",
       "\n",
       "                     optimization –\n",
       "               calculate_optimal –\n",
       "                 Training_DtA – \n",
       "\n",
       "                     new_net –\n",
       "\n",
       "                         loops –\n",
       "\n",
       "                         (training_data(Data)),\n",
       "\n",
       "                           Data,\n",
       "\n",
       "                       report –\n",
       "\n",
       "            calculation –\n",
       "                    calc –\n",
       "\n",
       "                         training_data\\Data)}\n",
       "\n",
       "         ]\n",
       "\n",
       "### Train the Model\n",
       "\n",
       "To train the model, we will use the `learn` method with an optimizer and a loss function.\n",
       "\n",
       "elixir\n",
       "env = %{\n",
       "  data: %{\n",
       "    title: \"Train the Model\",\n",
       "    content: \"This is where you'll train your model.\"\n",
       "  },\n",
       "}\n",
       "\n",
       "trainer_newernet(env)\n",
       "\n",
       "\n",
       "### Training Results\n",
       "\n",
       "To display the training results, we can use the `report` method with a format option.\n",
       "\n",
       "elixir\n",
       "report(env, format: :html)\n",
       "\n",
       "\n",
       "## Running Training Code Using iex\n",
       "\n",
       "Let's run some code in our iex session:\n",
       "\n",
       "elixir\n",
       "data = %{\"title\" => \"Run Training\", \"data\" => \"Hello, World!\"}\n",
       "trainer(data, learn:fn(env) -> io |> fn -> IO.puts \"Training in progress\"; end;)\n",
       "\n",
       "\n",
       "## Run Optimizer Over Data\n",
       "\n",
       "Let's define a simple training process that we can optimize with the `opt` and `optimization-optimize` helpers.\n",
       "\n",
       "elixir\n",
       "def run_training(optimizer, data) do\n",
       "  train = fn(env) ->\n",
       "    loop(fn(train_env) -> calc(train_env) end)\n",
       "  end\n",
       "  \n",
       "  train(&train/1,\n",
       "        optimization:fn(caller) ->\n",
       "          {optimizer.(caller), env}\n",
       "        end),\n",
       "        format: :json\n",
       "      )\n",
       "\n",
       "  result = run_training(optimizer, data)\n",
       "\n",
       "  case Enum.at([item | items], -1) do\n",
       "    {:ok, {:training_failed, _}} -> IO.puts \"Training failed.\"\n",
       "    {:ok, :training_succeeded} ->\n",
       "      IO.puts \"Success!\"\n",
       "    end\n",
       "end\n",
       "\n",
       "run_training(:adam, %{\"data\" => \"Hello, World!\"})\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "llama_asst = Assitant(model=MODEL_LLAMA)\n",
    "\n",
    "llama_asst.ask(user_prompt=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91d93031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, Erlang can be utilized in the field of machine learning (ML), even though it may not be as popular among ML tools due to its high concurrency features which play an integral role in many systems that require high availability or handle large numbers of concurrent requests, like e-commerce websites. However, with the right use case and through appropriate optimization techniques, Erlang can still provide immense benefits for machine learning purposes.\n",
       "\n",
       "1) Distributed Systems: One possible application is in developing fault-tolerant distributed systems to process data in real time. Such a system could distribute multiple instances of each node across different physical locations to ensure reliability even if one fails or slows down.\n",
       "\n",
       "2) Real Time Analytics and Decision Making: Advanced analytics use cases can benefit from Erlang's Actor Model for managing concurrency effectively. For example, real-time prediction models that make decisions based on streaming data in any given moment would be a good fit with its support for fault-tolerance & distribution among different instances of nodes across the system, to ensure efficient analysis at every stage/ stage.\n",
       "\n",
       "Creating an ML algorithm using Erlang can be tough considering its typical synchronous nature but we may leverage concurrent programming principles and async operations to design a distributed Erlang environment that supports incremental learning in real-time based on streaming data & make decisions about new model parameters accordingly.\n",
       "\n",
       "But, note here that implementing these applications often require advanced knowledge of ML (as well as statistics for some parts) plus functional programming with Erlang. It can get quite complicated and may even be difficult considering the lack of high-level libraries dedicated to neural networks in language like Python or R.\n",
       "\n",
       "As of examples on how implementation would look, training a simple MultiLayer Perceptron could be done using an erlang library called 'ErLee', which is Erlang bindings for Deeplearning4j Java library (a popular one for Neural Networks). But such examples are fairly basic and may require significant custom coding to get running. \n",
       "\n",
       "To start with, one should understand basic knowledge of ML, activation/loss functions, backpropagation for gradient descent optimization algorithm etc., before delving into Erlang implementation part. Therefore, in general cases a full-fledged MLaaS (machine learning as a service) wouldn't be implemented purely erlang and can potentially leverage 'nifs' (native interface from the language to an environment that supports them). \n",
       "\n",
       "So, for machine learning applications, it would typically involve using combination of languages/frameworks like Python plus libraries such as TensorFlow or PyTorch etc. which are more suited to this purpose and offer much higher degrees of freedom for implementing custom models with ease. However, these benefits in performance can be achieved by efficiently utilizing Erlang to take advantage from its concurrency & resilience features but a tradeoff for learning curve involved.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deepseek_asst = Assitant(model=\"deepseek-coder:6.7b-instruct\")\n",
    "deepseek_asst.ask(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
